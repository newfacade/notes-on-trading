# -*- coding: utf-8 -*-
import argparse
from datetime import datetime
import concurrent.futures
from typing import List, Dict, Callable, Union
import json
import re
import time
import concurrent.futures
import threading
import functools
import os
import random
import uuid
import sys

import requests
from tqdm import tqdm

# å…¨å±€æµå¼è¾“å‡ºé”ï¼Œç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªçº¿ç¨‹è¿›è¡Œæµå¼è¾“å‡º
stream_output_lock = threading.Lock()

parser = argparse.ArgumentParser()
parser.add_argument("-i", "--input_file", type=str, default="", help="è¾“å…¥æ–‡ä»¶")
parser.add_argument("-m", "--model", type=str, default="gpt-4-1106-preview", help="gptæ¨¡å‹ç‰ˆæœ¬")
parser.add_argument("--host", type=str, help="hostname")
parser.add_argument("--port", type=int, default=8000, help="port")

parser.add_argument("--topp", type=float, default=-1, help="topp")
parser.add_argument("--temperature", type=float, default=-1, help="temperature")
parser.add_argument("--num_threads", type=int, default=4, help="çº¿ç¨‹æ•°")
parser.add_argument("--is_random", default=False, action="store_true", help="æ˜¯å¦éšæœºé€‰æ‹©ä¸€ä¸ªtemperatureå’Œtopp")
parser.add_argument("--thinking", default=False, action="store_true", help="æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹")
parser.add_argument("--max_tokens", type=int, default=8192, help="æœ€å¤§tokenæ•°")
parser.add_argument("--stream", default=False, action="store_true", help="æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º")


def read_jsonl(filename: str):
    """
    Read a jsonl file (or a txt file), parse each line, and return a list.
    """
    with open(filename, "r", encoding="utf-8") as fp:
        return [json.loads(line) for line in fp]


def write_jsonl(filename: str, data: list):
    """
    Write iterable data to a jsonl file.
    """
    with open(filename, "w") as fp:
        for x in data:
            fp.write(json.dumps(x, ensure_ascii=False) + "\n")


def retry(max_attempts=10, delay=10):
    """
    Retry a function.
    """
    def decorator(func):
        # preserve the metadata of the original function when it is decorated
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            attempts = 0
            while attempts < max_attempts:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    attempts += 1
                    print(f"Occur {e}. Retrying...")
                    time.sleep(delay)
            return None
        return wrapper
    return decorator


def multi_thread_write(input_file: str, num_threads: int, process_func: Callable, output_file: str, **kwargs):
    """å¤šçº¿ç¨‹å†™æ–‡ä»¶

    Args:
        input_file (str): è¾“å…¥æ–‡ä»¶
        num_threads (int): çº¿ç¨‹æ•°
        process_func (Callable): å¤„ç†æ•°æ®çš„å‡½æ•°
        output_file (str): è¾“å‡ºæ–‡ä»¶
    """
    lst = read_jsonl(input_file)
    print(len(lst))
    for i, item in enumerate(lst):
        item['idx'] = i
    lock = threading.Lock()
    tmp_output_file = output_file + '-tmp.jsonl'

    def write_one_line(item):
        x = process_func(item, **kwargs)
        with lock:
            with open(tmp_output_file, "a") as fp:
                # éæµå¼æ¨¡å¼ä¸‹æ‰æ‰“å°å¤„ç†è¿›åº¦ï¼Œé¿å…ä¸æµå¼è¾“å‡ºå†²çª
                if not kwargs.get('stream', False):
                    print(f"process {item['idx']}")
                fp.write(json.dumps(x, ensure_ascii=False) + "\n")

    # ä½¿ç”¨çº¿ç¨‹æ± 
    futures = []
    with concurrent.futures.ThreadPoolExecutor(num_threads) as executor:
        for item in lst:
            future = executor.submit(write_one_line, item)
            futures.append(future)

    # æ¢å¤é¡ºåº
    result = read_jsonl(tmp_output_file)
    result.sort(key=lambda x: x['idx'])
    write_jsonl(output_file, result)
    os.remove(tmp_output_file)


@retry(max_attempts=20, delay=10)
def process_data(obj: dict, model: str, host: str, port=8000, topp=-1, temperature=-1, is_random=False, max_tokens=8192, stream=False):
    content = ""
    if 'query' in obj:
        content = obj['query']
    elif 'src' in obj:
        if isinstance(obj['src'], str):
            content = obj['src']
        elif isinstance(obj['src'], list):
            content = obj['src'][0]
    elif 'instruction' in obj:
        content = obj['instruction']
    assert content

    url = f"http://{host}:{port}/v1/chat/completions"

    headers = {
        "Content-Type": "application/json"
    }

    # è¯·æ±‚æ•°æ®
    data = {
        "model": model,  # ä½¿ç”¨ä½ æŒ‡å®šçš„æ¨¡å‹å
        "messages": [
            {
                "role": "user", 
                "content": content
            }
        ],
        "max_tokens": max_tokens,
        "temperature": 0.8 if temperature <= 0 else temperature,
        "top_p": 0.8 if topp <= 0 else topp,  # ä¿®å¤äº†è¿™é‡Œçš„bugï¼ŒåŸæ¥å†™çš„æ˜¯temperature
        # "repetition_penalty": 1.1
        "stream": stream  # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æµå¼è¾“å‡º
    }
    if is_random:
        data["temperature"] = random.uniform(0.2, 1.0)
        data["top_p"] = random.uniform(0.6, 1.0)

    if stream:
        # ä½¿ç”¨å…¨å±€é”ç¡®ä¿æµå¼è¾“å‡ºçš„è¿è´¯æ€§
        with stream_output_lock:
            # æµå¼è¾“å‡ºå¤„ç†
            print(f"\n{'='*60}")
            print(f"ğŸš€ å¼€å§‹å¤„ç†ç¬¬ {obj['idx']} æ¡æ•°æ®")
            print(f"ğŸ“ å†…å®¹é¢„è§ˆ: {content[:100]}{'...' if len(content) > 100 else ''}")
            print(f"{'='*60}")
            sys.stdout.flush()
            
            response = requests.post(url, headers=headers, json=data, timeout=3600, stream=True)
            response.raise_for_status()
            
            output = ""
            print(f"ã€ç¬¬ {obj['idx']} æ¡ã€‘æµå¼è¾“å‡ºå¼€å§‹ï¼š")
            print("-" * 40)
            sys.stdout.flush()
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        line = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                        if line.strip() == '[DONE]':
                            break
                        try:
                            chunk = json.loads(line)
                            if 'choices' in chunk and len(chunk['choices']) > 0:
                                delta = chunk['choices'][0].get('delta', {})
                                if 'content' in delta:
                                    content_chunk = delta['content']
                                    output += content_chunk
                                    print(content_chunk, end='', flush=True)  # å®æ—¶è¾“å‡ºæ¯ä¸ªchunk
                        except json.JSONDecodeError:
                            continue  # è·³è¿‡æ— æ³•è§£æçš„è¡Œ
            
            print(f"\n{'-'*40}")
            print(f"âœ… ã€ç¬¬ {obj['idx']} æ¡ã€‘æµå¼è¾“å‡ºå®Œæˆ (å…± {len(output)} å­—ç¬¦)")
            print(f"{'='*60}\n")
            sys.stdout.flush()
        
    else:
        # éæµå¼è¾“å‡ºå¤„ç†ï¼ˆåŸæ¥çš„é€»è¾‘ï¼‰
        response = requests.post(url, headers=headers, json=data, timeout=3600)
        response.raise_for_status()
        result = response.json()
        print(f"âœ… process {obj['idx']} completed")
        output = result["choices"][0]["message"]["content"]
    
    assert output
    obj['output'] = output
    obj['model_id'] = model
    obj['top_p'] = data['top_p']
    obj['temperature'] = data['temperature']
    obj['max_tokens'] = data['max_tokens']
    return obj


def gpt_fetch(input_file: str, num_threads: int, model: str, host: str, port=8000, topp=-1, temperature=-1, is_random=False, max_tokens=8192, stream=False):
    """
    å¤šçº¿ç¨‹è·å–gptçš„tgt

    Args:
        input_file (str): format {'query': ...}
        num_threads (int): çº¿ç¨‹æ•°
        model (str): gptæ¨¡å‹id
    """
    output_file = input_file.replace(
        ".jsonl", "") + "-" + datetime.now().strftime("%Y-%m-%d-%H-%M-%S") + "-" + str(topp) + '-'  + model + ".jsonl"
    # output_file = input_file.replace('.jsonl', f'-{model}.jsonl')
    print(output_file)
    
    if stream:
        print("ğŸŒŠ æµå¼è¾“å‡ºæ¨¡å¼å·²å¯ç”¨")
        print("ğŸ“ æç¤ºï¼šæµå¼è¾“å‡ºå°†æŒ‰é¡ºåºé€ä¸ªæ˜¾ç¤ºï¼Œç¡®ä¿è¾“å‡ºè¿è´¯æ€§")
        print("âš¡ åå°ä»ç„¶ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†ï¼Œä½†æ˜¾ç¤ºä¼šæ’é˜Ÿè¿›è¡Œ")
        print("="*60)
    
    multi_thread_write(input_file=input_file,
                       num_threads=num_threads,
                       process_func=process_data,
                       output_file=output_file,
                       model=model,
                       host=host,
                       port=port,
                       topp=topp, 
                       temperature=temperature,
                       is_random=is_random,
                       max_tokens=max_tokens,
                       stream=stream)
    return output_file


if __name__ == "__main__":
    args = parser.parse_args()
    gpt_fetch(args.input_file, args.num_threads, args.model, args.host, args.port, args.topp, args.temperature, args.is_random, args.max_tokens, args.stream) 